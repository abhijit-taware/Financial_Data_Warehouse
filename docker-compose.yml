version: "3.8"

services:
  # sqlserver:
  #   image: mcr.microsoft.com/mssql/server:2022-latest
  #   container_name: sqlserver
  #   environment:
  #     ACCEPT_EULA: "Y"
  #     SA_PASSWORD: "${MSSQL_SA_PASSWORD}"
  #     MSSQL_PID: "Express"  # Use Express edition
  #   ports:
  #     - "${MSSQL_PORT}:1433"
  #   volumes:
  #     - ./sqlserver/data:/var/opt/mssql
  #   networks:
  #     - analytics-net
  # sqlserver:
  #   image: mcr.microsoft.com/azure-sql-edge:latest
  #   container_name: sqlserver
  #   environment:
  #     ACCEPT_EULA: "1"
  #     SA_PASSWORD: "${MSSQL_SA_PASSWORD}"
  #   ports:
  #     - "${MSSQL_PORT}:1433"
  #   volumes:
  #     - ./sqlserver/data:/var/opt/mssql
  #   networks:
  #     - analytics-net
  #   cap_add:
  #     - SYS_PTRACE
  sqlserver:
    image: mcr.microsoft.com/mssql/server:2022-latest
    container_name: sqlserver
    environment:
      ACCEPT_EULA: "Y"
      SA_PASSWORD: "${MSSQL_SA_PASSWORD}"
      MSSQL_PID: "Express"
    ports:
      - "${MSSQL_PORT}:1433"
    networks:
      - analytics-net
    # NO VOLUMES - this often causes the LSA error

  postgres:
    image: postgres:15
    container_name: postgres
    environment:
      POSTGRES_USER: "${POSTGRES_USER}"
      POSTGRES_PASSWORD: "${POSTGRES_PASSWORD}"
      POSTGRES_DB: "${POSTGRES_DB}"
    ports:
      - "${POSTGRES_PORT}:5432"
    volumes:
      - ./postgres/data:/var/lib/postgresql/data
    networks:
      - analytics-net

  redis:
    image: redis:7.2
    container_name: redis
    ports:
      - "${REDIS_PORT}:6379"
    volumes:
      - ./redis/data:/data
    networks:
      - analytics-net

  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "${ZOOKEEPER_PORT}:2181"
    volumes:
      - ./zookeeper/data:/var/lib/zookeeper/data
    networks:
      - analytics-net

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "${KAFKA_PORT}:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      #KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:${KAFKA_PORT}
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:${KAFKA_PORT},PLAINTEXT_HOST://localhost:9093
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    volumes:
      - ./kafka/data:/var/lib/kafka/data
    networks:
      - analytics-net

  spark-master:
    image: bitnami/spark:3.5
    container_name: spark-master
    environment:
      - SPARK_MODE=master
    ports:
      - "${SPARK_MASTER_PORT}:7077"
      - "${SPARK_MASTER_WEB_PORT}:8080"
    volumes:
      - ./spark/scripts:/bitnami/spark
      - ./airflow/data:/opt/airflow/data
    networks:
      - analytics-net

  spark-worker:
    image: bitnami/spark:3.5
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:${SPARK_MASTER_PORT}
    ports:
      - "${SPARK_WORKER_WEB_PORT}:8081"
    volumes:
      - ./spark/scripts:/bitnami/spark
      - ./airflow/data:/opt/airflow/data
    networks:
      - analytics-net

  airflow-init:
    #image: apache/airflow:2.9.1-python3.10
    build:
      context: ./airflow
      dockerfile: Dockerfile
    container_name: airflow-init
    depends_on:
      - postgres
      - redis
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      _AIRFLOW_WWW_USER_USERNAME: "${AIRFLOW_USERNAME}"
      _AIRFLOW_WWW_USER_PASSWORD: "${AIRFLOW_PASSWORD}"
    user: "50000:50000"
    volumes:
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/plugins:/opt/airflow/plugins
      - ./jupyter:/opt/jupyter
      - ./airflow:/opt/airflow
      #- ./airflow/data:/opt/airflow/data
    entrypoint:
      - /bin/bash
      - -c
      - |
        airflow db upgrade
        airflow users create --username ${AIRFLOW_USERNAME} --password ${AIRFLOW_PASSWORD} --firstname Admin --lastname User --role Admin --email admin@example.com
    networks:
      - analytics-net

  airflow-webserver:
    #image: apache/airflow:2.9.1-python3.10
    build:
      context: ./airflow
      dockerfile: Dockerfile
    container_name: airflow-webserver
    depends_on:
      - airflow-init
      - postgres
      - redis
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      _AIRFLOW_WWW_USER_USERNAME: "${AIRFLOW_USERNAME}"
      _AIRFLOW_WWW_USER_PASSWORD: "${AIRFLOW_PASSWORD}"
      #_PIP_ADDITIONAL_REQUIREMENTS: "-r /requirements.txt"
    ports:
      - "${AIRFLOW_WEB_PORT}:8080"
    user: "50000:50000"
    volumes:
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/plugins:/opt/airflow/plugins
      #- ./airflow/requirements.txt:/requirements.txt
      - ./jupyter:/opt/jupyter
      - ./airflow:/opt/airflow
      #- ./airflow/data:/opt/airflow/data
    command: webserver
    networks:
      - analytics-net

  airflow-scheduler:
    #image: apache/airflow:2.9.1-python3.10
    build:
      context: ./airflow
      dockerfile: Dockerfile
    container_name: airflow-scheduler
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      #_PIP_ADDITIONAL_REQUIREMENTS: "-r /requirements.txt"
    command: scheduler
    user: "50000:50000"
    volumes:
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/plugins:/opt/airflow/plugins
      #- ./airflow/requirements.txt:/requirements.txt
      - ./jupyter:/opt/jupyter
      - ./airflow:/opt/airflow
      #- ./airflow/data:/opt/airflow/data
    networks:
      - analytics-net

  airflow-worker:
    #image: apache/airflow:2.9.1-python3.10
    build:
      context: ./airflow
      dockerfile: Dockerfile
    container_name: airflow-worker
    depends_on:
      - airflow-init
      - airflow-scheduler
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      #_PIP_ADDITIONAL_REQUIREMENTS: "-r /requirements.txt"
    command: celery worker
    user: "50000:50000"
    volumes:
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/plugins:/opt/airflow/plugins
      #- ./airflow/requirements.txt:/requirements.txt
      - ./jupyter:/opt/jupyter
      - ./airflow:/opt/airflow
      #- ./airflow/data:/opt/airflow/data
    networks:
      - analytics-net

  jupyter:
    #image: jupyter/pyspark-notebook:spark-3.5.0
    build:
      context: ./jupyter
      dockerfile: Dockerfile
    container_name: jupyter
    #environment:
      #_PIP_ADDITIONAL_REQUIREMENTS: "-r /requirements.txt"
    ports:
      - "${JUPYTER_PORT}:8888"
      - "4040:4040"
    volumes:
      - ./jupyter/notebooks:/home/jovyan/work
      - ./jupyter/data:/home/jovyan/data
      #- ./jupyter/requirements.txt:/requirements.txt
    networks:
      - analytics-net

  kafdrop:
    image: obsidiandynamics/kafdrop
    container_name: kafdrop
    ports:
      - "9000:9000"
    environment:
      KAFKA_BROKER_CONNECT: kafka:9092
      JVM_OPTS: "-Xms64M -Xmx512M"
    depends_on:
      - kafka
    networks:
      - analytics-net

  #directory-setup:
   #   image: alpine:latest
    #  container_name: directory-setup
     # volumes:
      #  - ./airflow/data:/opt/airflow/data:rw
      #command: >
       # sh -c "
        #  mkdir -p /opt/airflow/data/processed/crypto /opt/airflow/data/processed/stocks
        # mkdir -p /opt/airflow/data/checkpoints/crypto /opt/airflow/data/checkpoints/stocks
        #  mkdir -p /opt/airflow/data/raw/crypto /opt/airflow/data/raw/stocks
        #  mkdir -p /opt/airflow/data/logs
        #  chmod -R 755 /opt/airflow/data
        #  echo 'Directories created successfully'
        #"
      #networks:
       # - analytics-net

networks:
  analytics-net:
    driver: bridge

# No named volumes needed now since all are mapped to host folders
