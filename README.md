# ğŸ“Š Financial Data Warehouse

## ğŸ“Œ Project Overview

**Financial Data Warehouse** is a modular and automated data pipeline designed for ingesting, transforming, and storing real-time and daily financial market data (stocks and cryptocurrencies). It leverages **Apache Airflow**, **Kafka**, **PySpark**, and **Docker** to build a scalable, production-ready data platform.

---

## ğŸš€ Features

- Daily and real-time data ingestion of stock and crypto OHLCV data.
- Data transformation using Apache Spark for large-scale processing.
- Real-time data streaming with Kafka.
- Workflow orchestration using Apache Airflow.
- Fully containerised with Docker Compose for easy deployment.

---

## ğŸ—‚ï¸ Project Structure

Financial_Data_Warehouse/
â”œâ”€â”€ .env
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ How_to_run_pipeline.txt
â”œâ”€â”€ manual_directory_creation.txt
â”œâ”€â”€ airflow/
â”‚ â”œâ”€â”€ Dockerfile
â”‚ â”œâ”€â”€ airflow.cfg
â”‚ â”œâ”€â”€ requirements.txt
â”‚ â”œâ”€â”€ webserver_config.py
â”‚ â”œâ”€â”€ dags/
â”‚ â”‚ â””â”€â”€ daily_market_data_pipeline/
â”‚ â”‚ â”œâ”€â”€ daily_market_data_pipeline.py
â”‚ â”‚ â””â”€â”€ tasks/
â”‚ â”‚ â”œâ”€â”€ check_market_hours.py
â”‚ â”‚ â”œâ”€â”€ determine_extraction.py
â”‚ â”‚ â”œâ”€â”€ extract_crypto_data.py
â”‚ â”‚ â”œâ”€â”€ extract_stocks_data.py
â”‚ â”‚ â”œâ”€â”€ stream_data_using_kafka.py
â”‚ â”‚ â”œâ”€â”€ transform_crypto_data_spark.py
â”‚ â”‚ â”œâ”€â”€ transform_stock_data_spark.py
â”‚ â”‚ â”œâ”€â”€ validate_api_connections.py
â”‚ â”‚ â”œâ”€â”€ create_spark_default_conn.py
â”‚ â”‚ â””â”€â”€ path_and_permession_setup.py
â””â”€â”€ README.md


---

## âš™ï¸ Setup Instructions

### 1. Clone the Repository
```bash
git clone https://github.com/your-username/your-repo.git
cd your-repo

mkdir -p airflow/{logs, dags, plugins, data}
mkdir jupyter

docker-compose up --build


---

### âœ… Part 3: DAG & Tech Stack

```markdown
---

## ğŸ“Œ DAG Workflow Breakdown

**DAG Name**: `daily_market_data_pipeline`

| Task                        | Description                                  |
|----------------------------|----------------------------------------------|
| `check_market_hours`       | Ensures markets are open before extraction   |
| `validate_api_connections` | Validates health of APIs (e.g., Alpha Vantage) |
| `extract_stocks_data`      | Extracts OHLCV data for stock symbols        |
| `extract_crypto_data`      | Extracts OHLCV data for cryptocurrencies     |
| `stream_data_using_kafka`  | Streams raw data into Kafka topic            |
| `transform_stock_data_spark` | Processes and cleans stock data with PySpark |
| `transform_crypto_data_spark` | Processes and cleans crypto data with PySpark |

---

## ğŸ§± Tech Stack

- **Airflow** â€“ DAG scheduling and orchestration  
- **Kafka** â€“ Real-time streaming layer  
- **PySpark** â€“ Batch data transformation  
- **Docker** â€“ Containerized development environment  
- **PostgreSQL / Delta Lake** â€“ (Pluggable) data storage sinks  
